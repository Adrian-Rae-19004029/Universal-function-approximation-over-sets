{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adrian-Rae-19004029/Universal-function-approximation-over-sets/blob/main/Experiments/E2%20-%20Standard%20Deviation%20of%20Textual%20Elements/E2_STD_of_Scalar_Elements.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9etIixaolEa"
      },
      "source": [
        "# **Universal Function Approximation over Sets**\n",
        "## **Experiment 2:** *Population standard deviation of a set of scalar values*\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzHLprkw-FV"
      },
      "source": [
        "## Imports and Library Functions\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-IpXz2VNyxD"
      },
      "outputs": [],
      "source": [
        "# SYSTEM RELATED IMPORTS\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "from pathlib import Path\n",
        "import time\n",
        "!pip install Pympler\n",
        "from pympler import asizeof\n",
        "get_alloc = asizeof.asizeof\n",
        "\n",
        "# MATH RELATED IMPORTS\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# VIS RELATED IMPORTS\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TF/KERAS RELATED\n",
        "!pip install tensorflow==2.9.1\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Dense, LSTM, GRU, Embedding, Lambda, serialize, deserialize, Attention\n",
        "from keras.models import Model, load_model, clone_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# MISC IMPORTS\n",
        "from tqdm import tqdm,trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94GTe6iEfwaz"
      },
      "source": [
        "## Basic Settings\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_ZZaSzFsM4S"
      },
      "outputs": [],
      "source": [
        "datetime = int(time.time())\n",
        "seed = datetime\n",
        "write_results = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee25X0K8xYvL"
      },
      "source": [
        "## Global Experimental Parameters \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD-PBx5DOSMv"
      },
      "outputs": [],
      "source": [
        "# TRAINING SET PARAMETERS\n",
        "n_train = 100000 # number of training examples\n",
        "max_train = 10 # maximum cardinality of a training set member\n",
        "\n",
        "# TESTING SET PARAMETERS\n",
        "n_test = 5000 # number of testing examples\n",
        "min_test = 5 # minimum cardinality of a testing set member\n",
        "max_test = 100 # maximum cardinality of a testing set member\n",
        "step_test = 5 # interval through which cardinalities of set members are tested\n",
        "\n",
        "# SET FUNCTION TO APPROXIMATE\n",
        "# Maps an input set of variable size to a target label\n",
        "#================================\n",
        "labelling_function = np.std\n",
        "#================================\n",
        "\n",
        "# ELEMENT DISTRIBUTION\n",
        "# How an individual element of a set is generated\n",
        "#================================\n",
        "input_range = (0,9)\n",
        "element_generator = lambda: np.random.uniform(*input_range)\n",
        "#================================\n",
        "\n",
        "\n",
        "# REPEATABILITY\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# STORAGE & MISC\n",
        "data_path = '/tmp'\n",
        "weight_path = '/tmp'\n",
        "result_path = '/content/drive/MyDrive/FAOS_results'\n",
        "timing_path = result_path\n",
        "test_name = 'E2'\n",
        "always_regenerate_data = True\n",
        "always_regenerate_weights = True\n",
        "base_verbose = 1\n",
        "config_hash = \"Final\"\n",
        "plot_scale = 1.6\n",
        "plot_text = 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YttSGlXihB9F"
      },
      "source": [
        "## Model Aggregation Strategies\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06JU6or6OdgO"
      },
      "outputs": [],
      "source": [
        "class SummationAggregation(Lambda):  \n",
        "    def __init__(self, function=lambda x: K.sum(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
        "      super(SummationAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def get_layer_name(cls):\n",
        "      return \"Summation\"\n",
        "\n",
        "class ArithmeticMeanAggregation(Lambda):  \n",
        "    def __init__(self, function=lambda x: K.mean(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
        "      super(ArithmeticMeanAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def get_layer_name(cls):\n",
        "      return \"Arithmetic Mean\"\n",
        "\n",
        "class ProductAggregation(Lambda):  \n",
        "    def __init__(self, function=lambda x: K.prod(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
        "      super(ProductAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def get_layer_name(cls):\n",
        "      return \"Hadamard Product\"\n",
        "\n",
        "class MaximumAggregation(Lambda):  \n",
        "    def __init__(self, function=lambda x: K.max(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
        "      super(MaximumAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def get_layer_name(cls):\n",
        "      return \"Maximum\"\n",
        "\n",
        "class MinimumAggregation(Lambda):  \n",
        "    def __init__(self, function=lambda x: K.min(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
        "      super(MinimumAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def get_layer_name(cls):\n",
        "      return \"Minimum\"\n",
        "\n",
        "# class GeometricMeanAggregation(Lambda):  \n",
        "#     def __init__(self, function=lambda x: K.exp(K.mean(K.log(K.cast(x, tf.float64)), axis=1)), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
        "#       super(GeometricMeanAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
        "\n",
        "#     classmethod\n",
        "#     def get_layer_name(cls):\n",
        "#       return \"Geometric Mean\"\n",
        "\n",
        "# class SelfAttentionAggregation(Lambda):  \n",
        "#     def __init__(self, function=None, output_shape=None, mask=None, arguments=None, trainable=None, **kwargs):\n",
        "#       def self_attention(X):\n",
        "#         scores = tf.matmul(X,X,transpose_b=True)\n",
        "#         distribution = tf.nn.softmax(scores)\n",
        "#         return tf.matmul(distribution, X)\n",
        "      \n",
        "#       f = lambda x: K.sum(self_attention(x), axis=1)\n",
        "#       sh = (lambda shape: (shape[0], shape[2]))\n",
        "#       super(SelfAttentionAggregation, self).__init__(f, sh, mask=None, arguments=arguments, trainable=False, **kwargs)\n",
        "\n",
        "#     classmethod\n",
        "#     def get_layer_name(cls):\n",
        "#       return \"Scaled Dot Product Self Attention\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXqGLjXKguxp"
      },
      "source": [
        "## Model Hyper-parameters and Metrics\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAahJp4KgcvW"
      },
      "outputs": [],
      "source": [
        "# Establish the list of aggregation layers to test with\n",
        "aggregator_list = [SummationAggregation, ArithmeticMeanAggregation, ProductAggregation, MaximumAggregation, MinimumAggregation]\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "hyper_parameters = {\n",
        "    'aggregation': { # Aggregation layer properties\n",
        "      'Summation': {\n",
        "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2}, \n",
        "          'color': 'g',\n",
        "          'style': 'go-'\n",
        "      },\n",
        "      'Arithmetic Mean': {\n",
        "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2},\n",
        "          'color': 'r',\n",
        "          'style': 'ro-'\n",
        "      },\n",
        "      'Hadamard Product': {\n",
        "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2},\n",
        "          'color': 'b',\n",
        "          'style': 'bo-'\n",
        "      },\n",
        "      'Maximum': {\n",
        "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2},\n",
        "          'color': 'y',\n",
        "          'style': 'yo-'\n",
        "      },\n",
        "      'Minimum': {\n",
        "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2},\n",
        "          'color': 'm',\n",
        "          'style': 'mo-'\n",
        "      },  \n",
        "    },\n",
        "    'encoder': [Dense, Dense, Dense], # Encoder structure to be used by model: simple three layer NN\n",
        "    'encoder_args': [{'units': 100}, {'units': 30}, {'units': 10}], # Encoder arguments\n",
        "    'decoder': [Dense], # Encoder structure to be used by model: simple layer \n",
        "    'decoder_args': [{'units': 1}], # Decoder arguments\n",
        "    'p_validation': 0.15, # Proportion of training set used for validation\n",
        "    'n_epochs': 50, # Number of training epochs\n",
        "    'n_batch': 128, # Batch size for training / evaluation\n",
        "    'optimizer': Adam, # Optimizer\n",
        "    'loss': 'mae' # Loss function   \n",
        "}\n",
        "\n",
        "# Simple method to extract a hyper-parameter\n",
        "def get_hyperparameter(*args):\n",
        "  src = hyper_parameters\n",
        "  try:\n",
        "    for item in args:\n",
        "      src = src.get(item)\n",
        "  except Exception:\n",
        "    src = None\n",
        "  return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYbFNUDbh6LA"
      },
      "source": [
        "## Helper methods for saving files\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1qb1pIgUX8Q"
      },
      "outputs": [],
      "source": [
        "# Create a unique hash of a collection of items\n",
        "def hash_elements(*args):\n",
        "  n = len(args)\n",
        "  str_placements = n * '{}'\n",
        "  str_args = [str(arg) for arg in args]\n",
        "  plainstring = str_placements.format(*str_args)\n",
        "  return hash(plainstring)\n",
        "\n",
        "# Create a filename for the training set based on experimental configurations\n",
        "def training_set_filenames():\n",
        "  # Reusing an existing training dataset depends on the following arguments\n",
        "  depends_on = [seed, max_train, n_train, labelling_function]\n",
        "  \n",
        "  # Create a hash of depends\n",
        "  identifier = hash_elements(*depends_on)\n",
        "  filename_dataset = '{}/{}_training_dataset_({}).npy'.format(data_path, test_name, identifier)\n",
        "  filename_labels = '{}/{}_training_labels_({}).npy'.format(data_path, test_name, identifier)\n",
        "  return filename_dataset, filename_labels\n",
        "\n",
        "# Create a filename for a testing set based on experimental configurations\n",
        "def testing_set_filenames(length):\n",
        "  # Reusing an existing testing dataset depends on the following arguments\n",
        "  depends_on = [length, seed, n_test, labelling_function]\n",
        "  \n",
        "  identifier = hash_elements(*depends_on)\n",
        "  filename_dataset = '{}/{}_testing_dataset_({}).npy'.format(data_path, test_name, identifier)\n",
        "  filename_labels = '{}/{}_testing_labels_({}).npy'.format(data_path, test_name, identifier)\n",
        "  return filename_dataset, filename_labels\n",
        "\n",
        "# Create a filename for model weights based on experimental configurations\n",
        "def weight_filename(layer):\n",
        "  # Reusing an existing weight file depends on the following arguments\n",
        "  depends_on = [layer, seed, max_train, n_train, labelling_function]\n",
        "  identifier = hash_elements(*depends_on)\n",
        "  return '{}/{}_weights_({}).hdf5'.format(weight_path, test_name, identifier)\n",
        "\n",
        "# Create a filename for the result set based on experimental configurations\n",
        "def result_filename():\n",
        "  return '{}/{}_results_({}).npy'.format(result_path, test_name, config_hash)\n",
        "\n",
        "# Create a filename for the timesheet based on experimental configurations\n",
        "def timesheet_filename():\n",
        "  return '{}/{}_times_({}).npy'.format(result_path, test_name, config_hash)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper methods for timekeeping\n",
        "---"
      ],
      "metadata": {
        "id": "inYVOj7CjdS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple timer to monitor training / evaluation times\n",
        "class Timer:\n",
        "  def start(self):\n",
        "    self._time = time.time()\n",
        "\n",
        "  def elapsed(self):\n",
        "    return time.time() - self._time\n",
        "\n",
        "# Timekeeping metrics\n",
        "timesheet = {layer.get_layer_name(): {'training': None, 'testing':{}} for layer in aggregator_list} "
      ],
      "metadata": {
        "id": "2MXeIGYlji8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNg9n552xqFW"
      },
      "source": [
        "## Training/Testing Dataset Generation Methods\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omsLkhZ8woqh"
      },
      "outputs": [],
      "source": [
        "def create_train_data(num_examples, length):\n",
        "\n",
        "  # Start with an empty list of examples and labels\n",
        "  X = []\n",
        "  X_labels = []\n",
        "  \n",
        "  # For the desired number of training examples\n",
        "  for i in tqdm(range(num_examples), desc='Creating training examples of maximum length {}: '.format(length)):\n",
        "    \n",
        "    # Generate a random set cardianality\n",
        "    n = np.random.randint(1, length)\n",
        "\n",
        "    # Generate a random set and add to list of examples\n",
        "    target_set = [element_generator() for _ in range(n)]\n",
        "    target_label = labelling_function(target_set)\n",
        "    X.append(target_set)\n",
        "    X_labels.append(target_label)\n",
        "\n",
        "  return tf.ragged.constant(X), tf.constant(X_labels)\n",
        "\n",
        "def gen_test_data(n_examples, length):\n",
        "    # Start with an empty list of examples and labels\n",
        "    X = []\n",
        "    X_labels = []\n",
        "    \n",
        "    # For the desired number of training examples\n",
        "    for i in tqdm(range(n_examples), desc='Creating testing examples of length {}: '.format(length)):\n",
        "\n",
        "      # Generate a random set and add to list of examples\n",
        "      target_set = [element_generator() for _ in range(length)]\n",
        "      target_label = labelling_function(target_set)\n",
        "      X.append(target_set)\n",
        "      X_labels.append(target_label)\n",
        "\n",
        "    return tf.ragged.constant(X), tf.constant(X_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlWq6olEimSH"
      },
      "source": [
        "## Training Dataset Creation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8X8wpPYGnFL"
      },
      "outputs": [],
      "source": [
        "# Create training sets\n",
        "X_train, label_X_train = None, None\n",
        "\n",
        "# Get filenames for storing training data\n",
        "filename_dataset, filename_labels = training_set_filenames()\n",
        "\n",
        "# Determine if a saved set already exists, else create one\n",
        "temp_dataset, temp_labels = None, None\n",
        "data_file, label_file = Path(filename_dataset), Path(filename_labels)\n",
        "if not data_file.is_file() or not label_file.is_file() or always_regenerate_data:\n",
        "  temp_dataset, temp_labels = create_train_data(n_train, max_train)\n",
        "  np.save(filename_dataset, temp_dataset.numpy())\n",
        "  np.save(filename_labels, temp_labels.numpy())\n",
        "\n",
        "X_train, label_X_train = temp_dataset, temp_labels "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KMgRFKriqZf"
      },
      "source": [
        "## Testing Dataset Creation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boF-WgyXQK9E"
      },
      "outputs": [],
      "source": [
        "# Create a collection of testing sets for each desired set length\n",
        "testing_collection = {}\n",
        "for l in range(min_test, max_test+1, step_test):\n",
        "  \n",
        "  temp_dataset, temp_labels = None, None\n",
        "  filename_dataset, filename_labels = testing_set_filenames(l)\n",
        "\n",
        "  # Determine if a saved set already exists, else create one\n",
        "  data_file, label_file = Path(filename_dataset), Path(filename_labels)\n",
        "  if not data_file.is_file() or not label_file.is_file() or always_regenerate_data:\n",
        "    temp_dataset, temp_labels = gen_test_data(n_test, l)\n",
        "    np.save(filename_dataset, temp_dataset.numpy())\n",
        "    np.save(filename_labels, temp_labels.numpy())\n",
        "  \n",
        "  # Add to the testing collection\n",
        "  testing_collection[l] = (temp_dataset, temp_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIli7Lq61oRx"
      },
      "source": [
        "## Model Creation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y7geulcOuiW"
      },
      "outputs": [],
      "source": [
        "# Method to create an encoder-decoder style model with a variable aggregation layer\n",
        "def build_model(aggregator):\n",
        "  \n",
        "  # Input is retrieved: ragged inputs used due to variable operand lengths\n",
        "  input = Input(shape=[None], ragged=True)\n",
        "  x = tf.expand_dims(input, axis=2)\n",
        "\n",
        "  # The following layers all comprise the 'encoder' function of the model  \n",
        "  encoder_layers = get_hyperparameter('encoder')\n",
        "  encoder_layer_args = get_hyperparameter('encoder_args')\n",
        "  encoder_sequence = [layer(**layer_arg) for layer, layer_arg in zip(encoder_layers, encoder_layer_args)]\n",
        "  for layer in encoder_sequence:\n",
        "    x = layer(x)\n",
        "  encoded = x\n",
        "\n",
        "  # The elements of the input have now been mapped to some element in a latent space\n",
        "  # Such latent embeddings are now embedded according to the desired aggregation strategy\n",
        "  x = aggregator(encoded)\n",
        "\n",
        "  # The aggregation is decoded to produce the resultant output\n",
        "  decoder_layers = get_hyperparameter('decoder')\n",
        "  decoder_layer_args = get_hyperparameter('decoder_args')\n",
        "  decoder_sequence = [layer(**layer_arg) for layer, layer_arg in zip(decoder_layers, decoder_layer_args)]\n",
        "  for layer in decoder_sequence:\n",
        "    x = layer(x)\n",
        "  decoded = x\n",
        "\n",
        "  # The model is returned\n",
        "  return Model(inputs=input, outputs=decoded)\n",
        "\n",
        "# Helper method to copy the weights of one model and set them in another\n",
        "def duplicate_weights(out_model, in_model):\n",
        "  for out_layer, in_layer in zip(out_model.layers,in_model.layers):\n",
        "    in_layer.set_weights(out_layer.get_weights())\n",
        "  return in_model\n",
        "\n",
        "# Wrapper to produce a custom object to register with each model\n",
        "def get_custom_object(in_layer):\n",
        "  return {in_layer.__name__: in_layer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVaGZbYZ3T3Q"
      },
      "source": [
        "## Model Training\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVQOPwt8O7Sz"
      },
      "outputs": [],
      "source": [
        "# Create a timer\n",
        "training_timer = Timer()\n",
        "\n",
        "# load dataset for this iteration\n",
        "filename_dataset, filename_labels = training_set_filenames()\n",
        "X_train = tf.ragged.constant(np.load(filename_dataset, allow_pickle=True))\n",
        "label_X_train = tf.constant(np.load(filename_labels, allow_pickle=True))\n",
        "\n",
        "# Create validation set, true training set, from training set\n",
        "# This is given a pre-established validation proportion\n",
        "n_train_total, _ = X_train.shape\n",
        "I_train, I_val = train_test_split(range(n_train_total), test_size=get_hyperparameter('p_validation'))\n",
        "\n",
        "X_train_partial = tf.gather(X_train, indices=I_train)\n",
        "X_val = tf.gather(X_train, indices=I_val)\n",
        "\n",
        "label_X_train_partial = tf.gather(label_X_train, indices=I_train)\n",
        "label_X_val = tf.gather(label_X_train, indices=I_val)\n",
        "\n",
        "# For each type of aggregation in consideration\n",
        "for agg_layer in aggregator_list:\n",
        "\n",
        "  # Get the name of the layer\n",
        "  layer_name = agg_layer.get_layer_name()\n",
        "\n",
        "  # Create the relevant model with desired aggregation\n",
        "  model = build_model(aggregator=agg_layer())\n",
        "  \n",
        "  # Register the custom object (aggregation layer)\n",
        "  custom_objects = get_custom_object(agg_layer)\n",
        "  with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "\n",
        "      # Compile the model, set optimizer and loss function\n",
        "      model = tf.keras.models.clone_model(model)\n",
        "      v_opt_args = get_hyperparameter('aggregation',layer_name,'optimizer_args')\n",
        "      v_opt = get_hyperparameter('optimizer')(**v_opt_args)\n",
        "      v_los = get_hyperparameter('loss')\n",
        "      model.compile(loss=v_los, optimizer=v_opt)\n",
        "\n",
        "      # Train and save weights if they don't already exist\n",
        "      filename = weight_filename(layer_name)\n",
        "      weight_file = Path(filename)\n",
        "      if not weight_file.is_file() or always_regenerate_weights:\n",
        "        print(\"Training commencing with aggregation layer: {}...\".format(layer_name))\n",
        "\n",
        "        # Checkpoint desirable weights based on validation loss\n",
        "        checkpointer = ModelCheckpoint(\n",
        "            filepath=filename, \n",
        "            verbose=0, \n",
        "            save_best_only=True\n",
        "        )\n",
        "\n",
        "        # adapt learning rate near loss landscape plateau\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=20, min_lr=0.000001)\n",
        "\n",
        "        # start timer\n",
        "        training_timer.start()\n",
        "\n",
        "        # Fit model\n",
        "        model.fit(\n",
        "            x=X_train_partial, \n",
        "            y=label_X_train_partial, \n",
        "            epochs=get_hyperparameter('n_epochs'), \n",
        "            batch_size=get_hyperparameter('n_batch'),\n",
        "            shuffle=True, \n",
        "            validation_data=(X_val, label_X_val),\n",
        "            callbacks=[checkpointer, reduce_lr],\n",
        "            verbose=base_verbose\n",
        "        )\n",
        "\n",
        "        # Gather elapsed time\n",
        "        elapsed = training_timer.elapsed()\n",
        "        timesheet[layer_name]['training'] = elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBlGMk9X5Q0E"
      },
      "source": [
        "## Performance Measures\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMoJxkFJ-Jxq"
      },
      "outputs": [],
      "source": [
        "# Mean Absolute Error \n",
        "def mae(pred, labels):\n",
        "  diff_vector = np.abs(np.squeeze(pred) - labels)\n",
        "  return np.sum(diff_vector) / len(labels)\n",
        "\n",
        "# Root Mean Squared Error \n",
        "def rmse(pred, labels):\n",
        "  diff_vector = np.squeeze(pred) - labels\n",
        "  return np.sqrt(np.dot( diff_vector, diff_vector) / len(labels))\n",
        "\n",
        "performance_metrics = [mae, rmse]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J62ugzu36C5c"
      },
      "source": [
        "## Model Evaluation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asNU6x4dPFH5"
      },
      "outputs": [],
      "source": [
        "lengths = range(min_test, max_test+1, step_test)\n",
        "\n",
        "# Create a timer\n",
        "testing_timer = Timer()\n",
        "\n",
        "# Lookup of prescribed metrics\n",
        "metrics = {in_layer.get_layer_name(): {met.__name__: {} for met in performance_metrics} for in_layer in aggregator_list}\n",
        "  \n",
        "# For each aggregation layer\n",
        "for agg_layer in aggregator_list:\n",
        "  \n",
        "  layer_name = agg_layer.get_layer_name()\n",
        "  print(\"Prediction commencing with aggregation layer: {}\".format(layer_name))\n",
        "  \n",
        "  # Determine, for sets of a particular length\n",
        "  for l in lengths:\n",
        "    print('Evaluating at length: ', l)\n",
        "    K.clear_session()\n",
        "    \n",
        "    # Gather test data\n",
        "    filename_dataset, filename_labels = testing_set_filenames(l)\n",
        "    X_test = np.load(filename_dataset, allow_pickle=True)\n",
        "    label_X_test = np.load(filename_labels, allow_pickle=True)\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(aggregator=agg_layer())\n",
        "\n",
        "    # Load weights as determined through training\n",
        "    filename = weight_filename(layer_name)\n",
        "    \n",
        "    custom_objects = get_custom_object(agg_layer)\n",
        "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "      \n",
        "      # Allocate weights for future computation\n",
        "      temp_model = load_model(filename)\n",
        "      duplicate_weights(temp_model, model)\n",
        "\n",
        "      # Start timer\n",
        "      testing_timer.start()\n",
        "\n",
        "      # Perform prediction \n",
        "      preds = model.predict(\n",
        "          X_test, \n",
        "          batch_size=get_hyperparameter('n_batch'), \n",
        "          verbose=base_verbose\n",
        "      )\n",
        "\n",
        "      # Gather elapsed time\n",
        "      elapsed = testing_timer.elapsed()\n",
        "      timesheet[layer_name]['testing'][l] = elapsed\n",
        "\n",
        "    # Add to monitored metric log\n",
        "    for met in performance_metrics:\n",
        "      metrics[layer_name][met.__name__][l] = met(preds, label_X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxvBuLaqlhSj"
      },
      "source": [
        "## Saving Results\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wr1HRwerwfV"
      },
      "outputs": [],
      "source": [
        "if write_results:\n",
        "  \n",
        "  # Save the performance results\n",
        "  output_filename = result_filename()\n",
        "  result_file = Path(output_filename)\n",
        "  if not result_file.is_file():\n",
        "    np.save(output_filename, np.array([metrics]))\n",
        "  else:\n",
        "    result_log = np.load(output_filename, allow_pickle=True)\n",
        "    result_log = np.append(result_log, metrics)\n",
        "    np.save(output_filename, result_log)\n",
        "\n",
        "  # Save the timesheet results\n",
        "  output_filename = timesheet_filename()\n",
        "  time_file = Path(output_filename)\n",
        "  if not time_file.is_file():\n",
        "    np.save(output_filename, np.array([timesheet]))\n",
        "  else:\n",
        "    time_log = np.load(output_filename, allow_pickle=True)\n",
        "    time_log = np.append(time_log, timesheet)\n",
        "    np.save(output_filename, time_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydoBsmslqbax"
      },
      "source": [
        "## Result Visualisation\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYssvEL3qa8Q"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def plot_results(metric, xlabel='Set Cardinality', ylabel='', title='', ylim=None, added_operations=lambda plt: None):\n",
        "  font = {'size': plot_text}\n",
        "  matplotlib.rc('font', **font)\n",
        "  scale = plot_scale\n",
        "  plt.figure(figsize=(10*scale, 8*scale))\n",
        "\n",
        "  y_max = -math.inf\n",
        "  for agg_layer in aggregator_list:\n",
        "    layer_name = agg_layer.get_layer_name()\n",
        "    style = get_hyperparameter('aggregation',layer_name,'style')\n",
        "    color = get_hyperparameter('aggregation',layer_name,'color')\n",
        "    x = np.array(list(metrics[layer_name][metric].keys()))\n",
        "    y = np.array(list(metrics[layer_name][metric].values()))\n",
        "    y_max = max(y_max, np.max(y))\n",
        "    plt.plot(x, y, style, label=layer_name)\n",
        "\n",
        "  plt.axvspan(1, max_train, color='C1', alpha=0.3, label='Training Set Size Threshold')\n",
        "\n",
        "  plt.grid()\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(\"{} [n={}]\".format(title, n_test))\n",
        "  if ylim is not None:\n",
        "    plt.ylim(*ylim)\n",
        "  plt.xlim(min_test - step_test, max_test + step_test)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  added_operations(plt)\n",
        "  plt.show()\n",
        "\n",
        "def plot_testing_times(xlabel='Set Cardinality', ylabel='', title='', ylim=None, added_operations=lambda plt: None):\n",
        "  font = {'size': plot_text}\n",
        "  matplotlib.rc('font', **font)\n",
        "  scale = plot_scale\n",
        "  plt.figure(figsize=(10*scale, 8*scale))\n",
        "\n",
        "  y_max = -math.inf\n",
        "  for agg_layer in aggregator_list:\n",
        "    layer_name = agg_layer.get_layer_name()\n",
        "    color = get_hyperparameter('aggregation',layer_name,'color')\n",
        "    point_style = \"{}o\".format(color)\n",
        "    line_style = \"{}-\".format(color)\n",
        "    x = np.array(list(timesheet[layer_name]['testing'].keys()))\n",
        "    y = np.array(list(timesheet[layer_name]['testing'].values()))\n",
        "    y_max = max(y_max, np.max(y))\n",
        "    m, b = np.polyfit(x,y,1)\n",
        "    yhat = m * x + b\n",
        "\n",
        "    plt.plot(x, y, point_style, markersize=0.8)\n",
        "    plt.plot(x, yhat, line_style, label=layer_name)\n",
        "\n",
        "  plt.grid()\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(\"{} [n={}]\".format(title,n_test))\n",
        "  if ylim is not None:\n",
        "    plt.ylim(*ylim)\n",
        "  plt.xlim(min_test - step_test, max_test + step_test)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  added_operations(plt)\n",
        "  plt.show()\n",
        "\n",
        "def plot_training_times(xlabel='', ylabel='', title='', ylim=None, added_operations=lambda plt: None):\n",
        "  font = {'size': plot_text}\n",
        "  matplotlib.rc('font', **font)\n",
        "  scale = plot_scale\n",
        "  plt.figure(figsize=(10*scale, 8*scale))\n",
        "\n",
        "  obs = {layer.get_layer_name(): timesheet[layer.get_layer_name()]['training'] for layer in aggregator_list}\n",
        "  x = obs.keys()\n",
        "  y = obs.values()\n",
        "  y_min, y_max = np.min(list(y)), np.max(list(y))\n",
        "\n",
        "  plt.grid()\n",
        "  plt.bar(x, y)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(\"{} [n={}]\".format(title,n_train))\n",
        "  plt.ylim(0.95 * y_min, 1.05 * y_max)\n",
        "  if ylim is not None:\n",
        "    plt.ylim(*ylim)\n",
        "  plt.tight_layout()\n",
        "  added_operations(plt)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwuMBNQll4Yp"
      },
      "source": [
        "## Model MAE\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSePTmeY8FRG"
      },
      "outputs": [],
      "source": [
        "plot_results(metric='mae', ylabel='MAE', title='Model Mean Absolute Error per Testing Set Cardinality')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwTUFWXWl7sk"
      },
      "source": [
        "## Model RMSE\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7bezLgFKyLM"
      },
      "outputs": [],
      "source": [
        "plot_results(metric='rmse', ylabel='RMSE', title='Model Root Mean Squared Error per Testing Set Cardinality')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training Times\n",
        "---"
      ],
      "metadata": {
        "id": "q_Gv9zR4cYml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_training_times(title=\"Training Period per Model\", ylabel=\"Training Period (s)\")"
      ],
      "metadata": {
        "id": "nmuVhvkdfR0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation Times\n",
        "---"
      ],
      "metadata": {
        "id": "JU9QKAWhiZkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_testing_times(title=\"Evaluation Times per Testing Set Cardinality [n={}]\".format(n_test), ylabel=\"Evaluation Time (s)\")"
      ],
      "metadata": {
        "id": "PaKFBqUCbf0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Multiple Experiments\n",
        "---"
      ],
      "metadata": {
        "id": "ZLuxDtvL2mHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to summarise experimental observations in a tabular format to some prescribed degree of accuracy\n",
        "from prettytable import PrettyTable\n",
        "def print_summary_statistics(x, Y, model=None, places=3):\n",
        "  n_experiments, n_classes = Y.shape\n",
        "  N = n_experiments * n_classes\n",
        "\n",
        "  if model is not None:\n",
        "    print(\"[Statistics for Model: {}]\".format(model))\n",
        "\n",
        "  x_obs = np.array([x for _ in range(n_experiments)]).flatten()\n",
        "  y_obs = Y.flatten()\n",
        "  rho = np.round(np.corrcoef(x_obs, y_obs)[0, 1],places)\n",
        "\n",
        "  headers = ['Basic Measure','Value']\n",
        "  table = PrettyTable(headers)\n",
        "  table.add_row(['Total Experiments', n_experiments])\n",
        "  table.add_row(['Total Observations', N])\n",
        "  table.add_row(['Correlation', rho])\n",
        "  print(table)\n",
        "  \n",
        "  measures = {\n",
        "    'Mean': np.mean(Y, axis=0),\n",
        "    'Min': np.min(Y, axis=0),\n",
        "    'Max': np.max(Y, axis=0),\n",
        "    'Std': np.std(Y, axis=0),    \n",
        "  }\n",
        "  \n",
        "  headers = ['Aggregate Measure'] + [\"length {}\".format(l) for l in x]\n",
        "  content = [[measure] + list(np.round(measures[measure],places)) for measure in measures.keys()]\n",
        "  table = PrettyTable(headers)\n",
        "  for row in content:\n",
        "    table.add_row(row)\n",
        "\n",
        "  print(table)"
      ],
      "metadata": {
        "id": "H3_WW3RMtPlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to plot performance metric as a function of set cardinality\n",
        "def plot_aggregate_metrics(metric, xlabel='Set Cardinality', ylabel='', title='', ylim=None, verbose=False, added_operations=lambda plt: None):\n",
        "  result_set = result_filename()\n",
        "  observations = np.load(result_set, allow_pickle=True)\n",
        "  n_observations = len(observations)\n",
        "\n",
        "  font = {'size': plot_text}\n",
        "  matplotlib.rc('font', **font)\n",
        "\n",
        "  scale = plot_scale\n",
        "  plt.figure(figsize=(10*scale, 8*scale))\n",
        "\n",
        "  for agg_layer in aggregator_list:\n",
        "    layer_name = agg_layer.get_layer_name()\n",
        "    color = get_hyperparameter('aggregation',layer_name,'color')\n",
        "    point_style = \"{}o\".format(color)\n",
        "    line_style = \"{}-\".format(color)\n",
        "    \n",
        "    x = np.array([list(observations[iter][layer_name][metric].keys()) for iter in range(n_observations)])\n",
        "    y = np.array([list(observations[iter][layer_name][metric].values()) for iter in range(n_observations)])\n",
        "    rho = np.round(np.corrcoef(x.flatten(), y.flatten())[0, 1], 5)\n",
        "    label = \"{} [ρ={}]\".format(layer_name, rho)\n",
        "    if verbose:\n",
        "      print_summary_statistics(x[0, :], y, model=label, places=5)\n",
        "\n",
        "    plt.plot(x, y, point_style, markersize=0.8)\n",
        "\n",
        "    xflat = x[0, :]\n",
        "    m, b = np.polyfit(x.flatten(),y.flatten(),1)\n",
        "    yhat = m * xflat + b\n",
        "    plt.plot(xflat, yhat, line_style, label=label)\n",
        "\n",
        "  plt.axvspan(1, max_train, color='C1', alpha=0.3, label='Training Set Size Threshold')\n",
        "\n",
        "  plt.grid()\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(\"{} [N={}, n={}]\".format(title,n_observations,n_test))\n",
        "  if ylim is not None:\n",
        "    plt.ylim(*ylim)\n",
        "  plt.xlim(min_test - step_test, max_test + step_test)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  added_operations(plt)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "_UyDiulHlG38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to create a boxplot of training times for each model\n",
        "def plot_aggregate_training_times(xlabel='', ylabel='', title='', added_operations=lambda plt: None):  \n",
        "  result_set = timesheet_filename()\n",
        "  observations = np.load(result_set, allow_pickle=True)\n",
        "  n_observations = len(observations)\n",
        "\n",
        "  font = {'size': plot_text}\n",
        "  matplotlib.rc('font', **font)\n",
        "\n",
        "  scale = plot_scale\n",
        "  fig, ax = plt.subplots(figsize=(10*scale, 8*scale))\n",
        "\n",
        "  key_values = []\n",
        "  data_values = []\n",
        "  for agg_layer in aggregator_list:\n",
        "    key = agg_layer.get_layer_name()\n",
        "    values = [observations[iter][key]['training'] for iter in range(n_observations)]\n",
        "    key_values.append(key)\n",
        "    data_values.append(values)\n",
        "\n",
        "  ax.boxplot(data_values)\n",
        "  ax.set_xticklabels(key_values)\n",
        "  \n",
        "  plt.grid()\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(\"{} [N={}, n={}]\".format(title,n_observations,n_train))\n",
        "  plt.tight_layout()\n",
        "  added_operations(plt)\n",
        "  plt.show()\n",
        "\n",
        "# Method to plot and analyse model evaluation times as a function of set cardinality\n",
        "def plot_aggregate_testing_times(xlabel='Set Cardinality', ylabel='', title='', ylim=None, added_operations=lambda plt: None):\n",
        "  result_set = timesheet_filename()\n",
        "  observations = np.load(result_set, allow_pickle=True)\n",
        "  n_observations = len(observations)\n",
        "  \n",
        "  font = {'size': plot_text}\n",
        "  matplotlib.rc('font', **font)\n",
        "  scale = plot_scale\n",
        "  plt.figure(figsize=(10*scale, 8*scale))\n",
        "\n",
        "  y_max = -math.inf\n",
        "  for agg_layer in aggregator_list:\n",
        "    layer_name = agg_layer.get_layer_name()\n",
        "    color = get_hyperparameter('aggregation',layer_name,'color')\n",
        "    point_style = \"{}o\".format(color)\n",
        "    line_style = \"{}-\".format(color)\n",
        "    x = np.array([list(observations[iter][layer_name]['testing'].keys()) for iter in range(n_observations)]).flatten()\n",
        "    y = np.array([list(observations[iter][layer_name]['testing'].values()) for iter in range(n_observations)]).flatten()\n",
        "    y_max = max(y_max, np.max(y))\n",
        "    m, b = np.polyfit(x,y,1)\n",
        "    yhat = m * x + b\n",
        "\n",
        "    rho = np.round(np.corrcoef(x.flatten(), y.flatten())[0, 1], 5)\n",
        "    label = \"{} [ρ={}]\".format(layer_name, rho)\n",
        "\n",
        "    plt.plot(x, y, point_style, markersize=0.8)\n",
        "    plt.plot(x, yhat, line_style, label=label)\n",
        "\n",
        "  plt.grid()\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.title(\"{} [N={},n={}]\".format(title,n_observations,n_test))\n",
        "  if ylim is not None:\n",
        "    plt.ylim(*ylim)\n",
        "  plt.xlim(min_test - step_test, max_test + step_test)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  added_operations(plt)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "m8TbZaoGJVEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-Term Model Mean Average Error\n",
        "---"
      ],
      "metadata": {
        "id": "tRG-xvzeWawP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_aggregate_metrics('mae', ylabel='MAE', ylim=(0,10), title='Mean Average Error Per Testing Set Cardinality')"
      ],
      "metadata": {
        "id": "Kl3DxO_BpQ3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-Term Model Root Mean Squared Error\n",
        "---"
      ],
      "metadata": {
        "id": "gaMKPWQPWU12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_aggregate_metrics('rmse', ylabel='RMSE', ylim=(0,10), title='Root Mean Squared Error Per Testing Set Cardinality')"
      ],
      "metadata": {
        "id": "MjMCRl0eISBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-Term Model Training Periods\n",
        "---"
      ],
      "metadata": {
        "id": "zzIilte3XhVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_aggregate_training_times(xlabel='Aggregation Mechanism', ylabel='Training Period (s)', title='Training Period per Aggregation Mechanism')"
      ],
      "metadata": {
        "id": "KUgok28xNuab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-Term Model Evaluation Periods\n",
        "---"
      ],
      "metadata": {
        "id": "_ebN0LdSXmli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_aggregate_testing_times(xlabel='Set Cardinality', ylabel='Evaluation Period (s)', title='Evaluation Period per Testing Set Cardinality')"
      ],
      "metadata": {
        "id": "9EHgUZaAOseb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "E2 - STD of Scalar Elements.ipynb",
      "provenance": [],
      "mount_file_id": "1oDiECkKzLkQJ11_MsUWV5eRZOh3umvoB",
      "authorship_tag": "ABX9TyPQmRmrtgtGJXHOhE8kMFkL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}