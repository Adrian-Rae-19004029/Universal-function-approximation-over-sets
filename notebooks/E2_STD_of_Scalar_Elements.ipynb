{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9etIixaolEa"
   },
   "source": [
    "# **Universal Function Approximation over Sets**\n",
    "## **Experiment 2:** *Standard Deviation of a set of scalar values*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISzHLprkw-FV"
   },
   "source": [
    "## Imports and Library Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard-plugin-profile\n",
    "# !pip install tensorflow==2.9.1\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-IpXz2VNyxD"
   },
   "outputs": [],
   "source": [
    "# SYSTEM RELATED IMPORTS\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# MATH RELATED IMPORTS\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TF/KERAS RELATED\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, LSTM, GRU, Embedding, Lambda, serialize, deserialize, Attention\n",
    "from keras.models import Model, load_model, clone_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# MISC IMPORTS\n",
    "from tqdm import tqdm,trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94GTe6iEfwaz"
   },
   "source": [
    "## Basic Settings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_ZZaSzFsM4S"
   },
   "outputs": [],
   "source": [
    "datetime = int(time.time())\n",
    "seed = datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee25X0K8xYvL"
   },
   "source": [
    "## Global Experimental Parameters \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oD-PBx5DOSMv"
   },
   "outputs": [],
   "source": [
    "# TRAINING SET PARAMETERS\n",
    "n_train = 100000 # number of training examples\n",
    "max_train = 10 # maximum cardinality of a training set member\n",
    "\n",
    "# TESTING SET PARAMETERS\n",
    "n_test = 5000 # number of testing examples\n",
    "min_test = 5 # minimum cardinality of a testing set member\n",
    "max_test = 100 # maximum cardinality of a testing set member\n",
    "step_test = 5 # interval through which cardinalities of set members are tested\n",
    "\n",
    "# SET FUNCTION TO APPROXIMATE\n",
    "# Maps an input set of variable size to a target label\n",
    "#================================\n",
    "labelling_function = np.std\n",
    "#================================\n",
    "\n",
    "# ELEMENT DISTRIBUTION\n",
    "# How an individual element of a set is generated\n",
    "#================================\n",
    "input_range = (0,9)\n",
    "element_generator = lambda: np.random.uniform(*input_range)\n",
    "#================================\n",
    "\n",
    "# REPEATABILITY\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# STORAGE & MISC\n",
    "data_dir = '/tmp'\n",
    "weights_dir = '/tmp'\n",
    "logs_dir = '/tmp'\n",
    "test_name = 'E2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YttSGlXihB9F"
   },
   "source": [
    "## Model Aggregation Strategies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06JU6or6OdgO"
   },
   "outputs": [],
   "source": [
    "class SummationAggregation(Lambda):  \n",
    "    def __init__(self, function=lambda x: K.sum(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
    "      super(SummationAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_layer_name(cls):\n",
    "      return \"Summation\"\n",
    "\n",
    "class ArithmeticMeanAggregation(Lambda):  \n",
    "    def __init__(self, function=lambda x: K.mean(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
    "      super(ArithmeticMeanAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_layer_name(cls):\n",
    "      return \"Arithmetic Mean\"\n",
    "\n",
    "class ProductAggregation(Lambda):  \n",
    "    def __init__(self, function=lambda x: K.prod(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
    "      super(ProductAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_layer_name(cls):\n",
    "      return \"Product\"\n",
    "\n",
    "class MaximumAggregation(Lambda):  \n",
    "    def __init__(self, function=lambda x: K.max(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
    "      super(MaximumAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_layer_name(cls):\n",
    "      return \"Maximum\"\n",
    "\n",
    "class MinimumAggregation(Lambda):  \n",
    "    def __init__(self, function=lambda x: K.min(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])), mask=None, arguments=None, trainable=False, **kwargs):\n",
    "      super(MinimumAggregation, self).__init__(function, output_shape, mask=mask, arguments=arguments, trainable=trainable, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def get_layer_name(cls):\n",
    "      return \"Minimum\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXqGLjXKguxp"
   },
   "source": [
    "## Model Hyper-parameters and Metrics\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAahJp4KgcvW"
   },
   "outputs": [],
   "source": [
    "# Establish the list of aggregation layers to test with\n",
    "aggregator_list = [SummationAggregation, ArithmeticMeanAggregation, ProductAggregation, MaximumAggregation, MinimumAggregation]\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "hyper_parameters = {\n",
    "    'aggregation': { # Aggregation layer properties\n",
    "      SummationAggregation.get_layer_name(): {\n",
    "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2}, \n",
    "      },\n",
    "      ArithmeticMeanAggregation.get_layer_name(): {\n",
    "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2},\n",
    "      },\n",
    "      ProductAggregation.get_layer_name(): {\n",
    "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2},\n",
    "      },\n",
    "      MaximumAggregation.get_layer_name(): {\n",
    "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2},\n",
    "      },\n",
    "      MinimumAggregation.get_layer_name(): {\n",
    "          'optimizer_args': {'learning_rate':1e-3, 'epsilon': 1e-2},\n",
    "      },  \n",
    "    },\n",
    "    'encoder': [Dense, Dense, Dense], # Encoder structure to be used by model: simple three layer NN\n",
    "    'encoder_args': [{'units':100, 'activation':'tanh'}, {'units': 30, 'activation':'tanh'}, {'units': 10}], # Encoder arguments\n",
    "    'decoder': [Dense], # Encoder structure to be used by model: simple layer \n",
    "    'decoder_args': [{'units': 1}], # Decoder arguments\n",
    "    'p_validation': 0.15, # Proportion of training set used for validation\n",
    "    'n_epochs': 20, # Number of training epochs\n",
    "    'n_batch': 128, # Batch size for training / evaluation\n",
    "    'optimizer': Adam, # Optimizer\n",
    "    'loss': 'mae' # Loss function   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYbFNUDbh6LA"
   },
   "source": [
    "## Helper methods for saving files\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1qb1pIgUX8Q"
   },
   "outputs": [],
   "source": [
    "# Create a filename for the training set based on experimental configurations\n",
    "def training_set_filenames():\n",
    "  # Reusing an existing training dataset depends on the following arguments\n",
    "  filename_dataset = '{}/{}_training_dataset.npy'.format(data_dir, test_name)\n",
    "  filename_labels = '{}/{}_training_labels.npy'.format(data_dir, test_name)\n",
    "  return filename_dataset, filename_labels\n",
    "\n",
    "\n",
    "# Create a filename for a testing set based on experimental configurations\n",
    "def testing_set_filenames(length):\n",
    "  # Reusing an existing testing dataset depends on the following arguments\n",
    "  filename_dataset = '{}/{}_testing_dataset_{}.npy'.format(data_dir, test_name, length)\n",
    "  filename_labels = '{}/{}_testing_labels_{}.npy'.format(data_dir, test_name, length)\n",
    "  return filename_dataset, filename_labels\n",
    "\n",
    "\n",
    "# Create a filename for model weights based on experimental configurations\n",
    "def weights_filename(layer):\n",
    "  # Reusing an existing weight file depends on the following arguments\n",
    "  return '{}/{}_weights_({}).hdf5'.format(weights_dir, test_name, layer)\n",
    "\n",
    "\n",
    "# Create a filename for the results stored in log\n",
    "def log_filename():\n",
    "  return '{}/{}_instance_({}).json'.format(logs_dir, test_name, seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log of Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_log = {layer.get_layer_name(): {\n",
    "    'loss_per_training_epoch': {},\n",
    "    'performance_per_set_size': {},\n",
    "    'training_time': None,\n",
    "    'evaluation_time_per_set_size':{}\n",
    "} for layer in aggregator_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inYVOj7CjdS1",
    "tags": []
   },
   "source": [
    "## Helper methods for timekeeping\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MXeIGYlji8Y"
   },
   "outputs": [],
   "source": [
    "# Simple timer to monitor training / evaluation times\n",
    "class Timer:\n",
    "  def start(self):\n",
    "    self._time = time.time()\n",
    "\n",
    "  def elapsed(self):\n",
    "    return time.time() - self._time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNg9n552xqFW"
   },
   "source": [
    "## Training/Testing Dataset Generation Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omsLkhZ8woqh"
   },
   "outputs": [],
   "source": [
    "def create_train_data(num_examples, length):\n",
    "\n",
    "  # Start with an empty list of examples and labels\n",
    "  X = []\n",
    "  X_labels = []\n",
    "  \n",
    "  # For the desired number of training examples\n",
    "  for i in tqdm(range(num_examples), desc='Creating training examples of maximum length {}: '.format(length)):\n",
    "    \n",
    "    # Generate a random set cardianality\n",
    "    n = np.random.randint(1, length)\n",
    "\n",
    "    # Generate a random set and add to list of examples\n",
    "    target_set = [element_generator() for _ in range(n)]\n",
    "    target_label = labelling_function(target_set)\n",
    "    X.append(target_set)\n",
    "    X_labels.append(target_label)\n",
    "\n",
    "  return tf.ragged.constant(X), tf.constant(X_labels)\n",
    "\n",
    "def gen_test_data(n_examples, length):\n",
    "    # Start with an empty list of examples and labels\n",
    "    X = []\n",
    "    X_labels = []\n",
    "    \n",
    "    # For the desired number of training examples\n",
    "    for i in tqdm(range(n_examples), desc='Creating testing examples of length {}: '.format(length)):\n",
    "\n",
    "      # Generate a random set and add to list of examples\n",
    "      target_set = [element_generator() for _ in range(length)]\n",
    "      target_label = labelling_function(target_set)\n",
    "      X.append(target_set)\n",
    "      X_labels.append(target_label)\n",
    "\n",
    "    return tf.ragged.constant(X), tf.constant(X_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlWq6olEimSH"
   },
   "source": [
    "## Training Dataset Creation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8X8wpPYGnFL",
    "outputId": "c0e6d805-8046-42c0-a311-876583632c9f"
   },
   "outputs": [],
   "source": [
    "# Create training sets\n",
    "X_train, label_X_train = None, None\n",
    "\n",
    "# Get filenames for storing training data\n",
    "filename_dataset, filename_labels = training_set_filenames()\n",
    "\n",
    "# Determine if a saved set already exists, else create one\n",
    "temp_dataset, temp_labels = None, None\n",
    "data_file, label_file = Path(filename_dataset), Path(filename_labels)\n",
    "temp_dataset, temp_labels = create_train_data(n_train, max_train)\n",
    "np.save(filename_dataset, temp_dataset.numpy())\n",
    "np.save(filename_labels, temp_labels.numpy())\n",
    "\n",
    "X_train, label_X_train = temp_dataset, temp_labels \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KMgRFKriqZf"
   },
   "source": [
    "## Testing Dataset Creation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boF-WgyXQK9E",
    "outputId": "7927d445-4d9b-486c-f2ef-c32ec03f1c5c"
   },
   "outputs": [],
   "source": [
    "# Create a collection of testing sets for each desired set length\n",
    "testing_collection = {}\n",
    "for l in range(min_test, max_test+1, step_test):\n",
    "  \n",
    "  temp_dataset, temp_labels = None, None\n",
    "  filename_dataset, filename_labels = testing_set_filenames(l)\n",
    "\n",
    "  # Determine if a saved set already exists, else create one\n",
    "  data_file, label_file = Path(filename_dataset), Path(filename_labels)\n",
    "  temp_dataset, temp_labels = gen_test_data(n_test, l)\n",
    "  np.save(filename_dataset, temp_dataset.numpy())\n",
    "  np.save(filename_labels, temp_labels.numpy())\n",
    "  \n",
    "  # Add to the testing collection\n",
    "  testing_collection[l] = (temp_dataset, temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIli7Lq61oRx"
   },
   "source": [
    "## Model Creation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y7geulcOuiW"
   },
   "outputs": [],
   "source": [
    "# Method to create an encoder-decoder style model with a variable aggregation layer\n",
    "def build_model(aggregator):\n",
    "  \n",
    "  # Input is retrieved: ragged inputs used due to variable operand lengths\n",
    "  input = Input(shape=[None], ragged=True)\n",
    "  x = tf.expand_dims(input, axis=2)\n",
    "\n",
    "  # The following layers all comprise the 'encoder' function of the model  \n",
    "  encoder_layers = hyper_parameters.get('encoder')\n",
    "  encoder_layer_args = hyper_parameters.get('encoder_args')\n",
    "  encoder_sequence = [layer(**layer_arg) for layer, layer_arg in zip(encoder_layers, encoder_layer_args)]\n",
    "  for layer in encoder_sequence:\n",
    "    x = layer(x)\n",
    "  encoded = x\n",
    "\n",
    "  # The elements of the input have now been mapped to some element in a latent space\n",
    "  # Such latent embeddings are now embedded according to the desired aggregation strategy\n",
    "  x = aggregator(encoded)\n",
    "\n",
    "  # The aggregation is decoded to produce the resultant output\n",
    "  decoder_layers = hyper_parameters.get('decoder')\n",
    "  decoder_layer_args = hyper_parameters.get('decoder_args')\n",
    "  decoder_sequence = [layer(**layer_arg) for layer, layer_arg in zip(decoder_layers, decoder_layer_args)]\n",
    "  for layer in decoder_sequence:\n",
    "    x = layer(x)\n",
    "  decoded = x\n",
    "\n",
    "  # The model is returned\n",
    "  return Model(inputs=input, outputs=decoded)\n",
    "\n",
    "# Helper method to copy the weights of one model and set them in another\n",
    "def duplicate_weights(out_model, in_model):\n",
    "  for out_layer, in_layer in zip(out_model.layers,in_model.layers):\n",
    "    in_layer.set_weights(out_layer.get_weights())\n",
    "  return in_model\n",
    "\n",
    "# Wrapper to produce a custom object to register with each model\n",
    "def get_custom_object(in_layer):\n",
    "  return {in_layer.__name__: in_layer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVaGZbYZ3T3Q"
   },
   "source": [
    "## Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVQOPwt8O7Sz",
    "outputId": "834520c7-bb01-4ce1-b071-82224d55838a"
   },
   "outputs": [],
   "source": [
    "# Create a timer\n",
    "training_timer = Timer()\n",
    "\n",
    "# load dataset for this iteration\n",
    "filename_dataset, filename_labels = training_set_filenames()\n",
    "X_train = tf.ragged.constant(np.load(filename_dataset, allow_pickle=True))\n",
    "label_X_train = tf.constant(np.load(filename_labels, allow_pickle=True))\n",
    "\n",
    "# Create validation set, true training set, from training set\n",
    "# This is given a pre-established validation proportion\n",
    "n_train_total, _ = X_train.shape\n",
    "I_train, I_val = train_test_split(range(n_train_total), test_size=hyper_parameters.get('p_validation'))\n",
    "\n",
    "X_train_partial = tf.gather(X_train, indices=I_train)\n",
    "X_val = tf.gather(X_train, indices=I_val)\n",
    "\n",
    "label_X_train_partial = tf.gather(label_X_train, indices=I_train)\n",
    "label_X_val = tf.gather(label_X_train, indices=I_val)\n",
    "\n",
    "# For each type of aggregation in consideration\n",
    "for agg_layer in aggregator_list:\n",
    "\n",
    "  # Get the name of the layer\n",
    "  layer_name = agg_layer.get_layer_name()\n",
    "\n",
    "  # Create the relevant model with desired aggregation\n",
    "  model = build_model(aggregator=agg_layer())\n",
    "  \n",
    "  # Register the custom object (aggregation layer)\n",
    "  custom_objects = get_custom_object(agg_layer)\n",
    "  with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "\n",
    "      # Compile the model, set optimizer and loss function\n",
    "      model = tf.keras.models.clone_model(model)\n",
    "      v_opt_args = hyper_parameters.get('aggregation').get(layer_name).get('optimizer_args')\n",
    "      v_opt = hyper_parameters.get('optimizer')(**v_opt_args)\n",
    "      v_los = hyper_parameters.get('loss')\n",
    "      model.compile(loss=v_los, optimizer=v_opt)\n",
    "\n",
    "      # Train and save weights if they don't already exist\n",
    "      filename = weights_filename(layer_name)\n",
    "      weight_file = Path(filename)\n",
    "      if not weight_file.is_file():\n",
    "        print(\"Training commencing with aggregation layer: {}...\".format(layer_name))\n",
    "\n",
    "        # Checkpoint desirable weights based on validation loss\n",
    "        checkpointer = ModelCheckpoint(\n",
    "            filepath=filename, \n",
    "            verbose=0, \n",
    "            save_best_only=True\n",
    "        )\n",
    "\n",
    "        # start timer\n",
    "        training_timer.start()\n",
    "    \n",
    "        # =======================================\n",
    "        # pre-training fitness\n",
    "        epoch_0 = model.evaluate(x=X_val, y=label_X_val, batch_size=hyper_parameters.get('n_batch'))\n",
    "        \n",
    "        # Fit model\n",
    "        history = model.fit(\n",
    "            x=X_train_partial, \n",
    "            y=label_X_train_partial, \n",
    "            epochs=hyper_parameters.get('n_epochs'), \n",
    "            batch_size=hyper_parameters.get('n_batch'),\n",
    "            shuffle=True, \n",
    "            validation_data=(X_val, label_X_val),\n",
    "            callbacks=[\n",
    "                checkpointer, \n",
    "                #tboard_callback\n",
    "            ],\n",
    "            verbose=1\n",
    "        )\n",
    "        # =======================================\n",
    "\n",
    "        # Gather elapsed time\n",
    "        elapsed = training_timer.elapsed()\n",
    "        instance_log[layer_name]['training_time'] = elapsed\n",
    "        \n",
    "        # save training metrics\n",
    "        for epoch, value in enumerate([epoch_0] + history.history['val_loss']):     \n",
    "            instance_log[layer_name]['loss_per_training_epoch'][epoch] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBlGMk9X5Q0E"
   },
   "source": [
    "## Performance Measures\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMoJxkFJ-Jxq"
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def acc(pred, labels):\n",
    "  return 1.0 * np.sum(np.squeeze(np.round(preds)) == labels) / len(labels)\n",
    "\n",
    "# Mean Absolute Error \n",
    "def mae(pred, labels):\n",
    "  diff_vector = np.abs(np.squeeze(pred) - labels)\n",
    "  return np.sum(diff_vector) / len(labels)\n",
    "\n",
    "# Root Mean Squared Error \n",
    "def rmse(pred, labels):\n",
    "  diff_vector = np.squeeze(pred) - labels\n",
    "  return np.sqrt(np.dot( diff_vector, diff_vector) / len(labels))\n",
    "\n",
    "performance_metrics = [mae, rmse, acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J62ugzu36C5c"
   },
   "source": [
    "## Model Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asNU6x4dPFH5",
    "outputId": "fb118606-db73-4e0d-ca51-4f6269ee555d"
   },
   "outputs": [],
   "source": [
    "lengths = range(min_test, max_test+1, step_test)\n",
    "\n",
    "# Create a timer\n",
    "testing_timer = Timer()\n",
    "  \n",
    "# For each aggregation layer\n",
    "for agg_layer in aggregator_list:\n",
    "  \n",
    "  layer_name = agg_layer.get_layer_name()\n",
    "  print(\"Prediction commencing with aggregation layer: {}\".format(layer_name))\n",
    "  \n",
    "  # Add performance metrics\n",
    "  instance_log[layer_name]['performance_per_set_size'] = {met.__name__: {} for met in performance_metrics}\n",
    "  \n",
    "  # Determine, for sets of a particular length\n",
    "  for l in lengths:\n",
    "    print('Evaluating at length: ', l)\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Gather test data\n",
    "    filename_dataset, filename_labels = testing_set_filenames(l)\n",
    "    X_test = np.load(filename_dataset, allow_pickle=True)\n",
    "    label_X_test = np.load(filename_labels, allow_pickle=True)\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(aggregator=agg_layer())\n",
    "\n",
    "    # Load weights as determined through training\n",
    "    filename = weights_filename(layer_name)\n",
    "    \n",
    "    custom_objects = get_custom_object(agg_layer)\n",
    "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "      \n",
    "      # Allocate weights for future computation\n",
    "      temp_model = load_model(filename)\n",
    "      duplicate_weights(temp_model, model)\n",
    "\n",
    "      # Start timer\n",
    "      testing_timer.start()\n",
    "\n",
    "      # Perform prediction \n",
    "      preds = model.predict(\n",
    "          X_test, \n",
    "          batch_size=hyper_parameters.get('n_batch'), \n",
    "          verbose=1\n",
    "      )\n",
    "\n",
    "      # Gather elapsed time\n",
    "      elapsed = testing_timer.elapsed()\n",
    "      instance_log[layer_name]['evaluation_time_per_set_size'][l] = elapsed\n",
    "\n",
    "    # Add to monitored metric log\n",
    "    for met in performance_metrics:\n",
    "      performance = met(preds, label_X_test)\n",
    "      instance_log[layer_name]['performance_per_set_size'][met.__name__][l] = performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxvBuLaqlhSj"
   },
   "source": [
    "## Saving Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wr1HRwerwfV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the performance results\n",
    "output_filename = log_filename()\n",
    "with open(output_filename, 'w') as handle:\n",
    "    json.dump(instance_log, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove temporary data / weight files\n",
    "for d, f in [(data_dir, data) for data in os.listdir(data_dir)] + [(weights_dir, weight) for weight in os.listdir(weights_dir)]:\n",
    "    path = \"{}/{}\".format(d,f)\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods to delete observations\n",
    "# os.remove(result_filename())\n",
    "# os.remove(training_results_filename())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
